{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "JyCWVhldy--r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import sqlite3\n",
        "import tiktoken\n",
        "import math\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import base64"
      ],
      "metadata": {
        "id": "e_ALjcc_zBYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# GitHub-friendly CONFIG\n",
        "# =========================\n",
        "from pathlib import Path\n",
        "import os\n",
        "import openai\n",
        "\n",
        "# Project root (default: current directory). You can override with an env var.\n",
        "PROJECT_DIR = Path(os.getenv(\"PFAS_PROJECT_DIR\", \".\")).resolve()\n",
        "\n",
        "# Put inputs (e.g., SQLite DB, term lists) under ./data\n",
        "DATA_DIR = PROJECT_DIR / \"data\"\n",
        "\n",
        "# All generated outputs (tables/figures) go under ./outputs\n",
        "OUTPUT_DIR = PROJECT_DIR / \"outputs\"\n",
        "FIG_DIR = OUTPUT_DIR / \"figures\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Main SQLite database expected at: ./data/PFAS_TK_articles_data.db\n",
        "DB_PATH = DATA_DIR / \"PFAS_TK_articles_data.db\"\n",
        "\n",
        "# Optional: default dataset input expected at: ./data/pfas_dataset.csv\n",
        "PFAS_DATASET_CSV = DATA_DIR / \"pfas_dataset.csv\"\n",
        "\n",
        "# -------------------------\n",
        "# LLM configuration (NO hard-coded keys)\n",
        "# -------------------------\n",
        "# Set these as environment variables before running:\n",
        "#   LLM_API_KEY   (required)\n",
        "#   LLM_BASE_URL  (optional; e.g., UF endpoint)\n",
        "#   LLM_MODEL     (optional; default below)\n",
        "LLM_API_KEY = os.getenv(\"LLM_API_KEY\")\n",
        "LLM_BASE_URL = os.getenv(\"LLM_BASE_URL\")  # e.g., LLM_BASE_URL if LLM_BASE_URL else None\n",
        "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
        "\n",
        "if not LLM_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"Missing LLM_API_KEY env var. Set it before running (do NOT paste keys into the notebook).\"\n",
        "    )\n",
        "\n",
        "client_kwargs = {'api_key': LLM_API_KEY}\n",
        "if LLM_BASE_URL:\n",
        "    client_kwargs['base_url'] = LLM_BASE_URL\n",
        "client = openai.OpenAI(**client_kwargs)"
      ],
      "metadata": {
        "id": "SdNffdqEHYYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a Researcher that extracts relevant data from research papers relating to Toxicokinetics\"},\n",
        "    {\"role\": \"user\", \"content\": \"1+1+54=\"}\n",
        "  ]\n",
        "\n",
        "# Use the new API interface\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=LLM_MODEL,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n",
        "\n",
        "if(completion):\n",
        "    messages.append(completion.choices[0].message)\n",
        "    messages.append({\"role\": \"user\", \"content\": \"1-1+34=\"})\n",
        "    completion = client.chat.completions.create(\n",
        "      model=LLM_MODEL,\n",
        "      messages=messages\n",
        "    )\n",
        "    print(completion.choices[0].message)\n",
        "    print(completion)"
      ],
      "metadata": {
        "id": "7iVHZ4YvHaJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a Researcher that tells if a research papers is relatied to Toxicokinetics or not\"},\n",
        "    {\"role\": \"user\", \"content\": \"1+1+54=\"}\n",
        "  ]\n",
        "\n",
        "# Use the new API interface\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=LLM_MODEL,\n",
        "  messages=messages\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)\n",
        "\n",
        "if(completion):\n",
        "    messages.append(completion.choices[0].message)\n",
        "    messages.append({\"role\": \"user\", \"content\": \"1-1+34=\"})\n",
        "    completion = client.chat.completions.create(\n",
        "      model=LLM_MODEL,\n",
        "      messages=messages\n",
        "    )\n",
        "    print(completion.choices[0].message)\n",
        "    print('----')\n",
        "    print(completion)"
      ],
      "metadata": {
        "id": "SwcNFTUnHbYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking relevance"
      ],
      "metadata": {
        "id": "RVXdg-o_HeWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ChatGptApi(messages):\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=LLM_MODEL,\n",
        "      messages=messages,\n",
        "        temperature=0,\n",
        "        max_tokens=10,\n",
        "    )\n",
        "    return completion.choices[0]"
      ],
      "metadata": {
        "id": "fKjstuwRHfPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_1=\"\"\"Given the title and abstract of a research paper, determine if the paper is relevant to the fields of toxicokinetics or pharmacokinetics.\n",
        "\n",
        "**Toxicokinetics** involves the study of how toxic substances are absorbed, distributed, metabolized, and excreted in living organisms. This includes:\n",
        "- Measuring concentrations of toxic substances in biological tissues over time.\n",
        "- Modeling how these substances move through the body.\n",
        "- Studying the effects of these substances on living organisms.\n",
        "\n",
        "**Pharmacokinetics** involves the study of how drugs are absorbed, distributed, metabolized, and excreted in living organisms. This includes:\n",
        "- Measuring concentrations of drugs in biological tissues over time.\n",
        "- Modeling how these drugs move through the body.\n",
        "- Studying the effects of these drugs on living organisms.\n",
        "\n",
        "**Relevance Criteria:** A paper is considered relevant to toxicokinetics or pharmacokinetics if it includes any of the following:\n",
        "- Discussions on the absorption, distribution, metabolism, or excretion of substances (whether toxic or drugs) in living organisms.\n",
        "- Measurements or models of concentrations of these substances in biological tissues over time.\n",
        "- Effects of these substances on living organisms.\n",
        "- Bioaccumulation, biomagnification, or movement through food webs.\n",
        "- Impact on health risks or safety due to the presence of these substances in the environment or organisms.\n",
        "- Any involvement of toxic substances (e.g., PFAS) or drugs in biological tissues or organisms, even if the primary focus is on environmental impact or remediation.\n",
        "- PFAS is involved\n",
        "\n",
        "Title: [Insert Paper Title Here]\n",
        "\n",
        "Abstract: [Insert Paper Abstract Here]\n",
        "\n",
        "Make the program high recall, it's fine if I have false positives but I want to get all the Yes(cover all articles).\n",
        "\n",
        "Based on these definitions and criteria, is this paper relevant to toxicokinetics or pharmacokinetics or involvement of toxins (PFAS) or PFAS involved or biological tissues? Answer 'Yes' if the content aligns with these fields, otherwise answer 'No.'\"\"\""
      ],
      "metadata": {
        "id": "1GJ2DNsbHgkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "# set up client once\n",
        "client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL if LLM_BASE_URL else None)\n",
        "\n",
        "def isRelevantPaper(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Researcher that tells if a research paper is related to Toxicokinetics of PFAS compounds or not.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    tries = 0\n",
        "    while tries < 3:   # max retries\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=LLM_MODEL,\n",
        "                messages=messages,\n",
        "                temperature=0\n",
        "            )\n",
        "            result = completion.choices[0].message.content.lower().strip()\n",
        "            print(\"Model output:\", result)\n",
        "\n",
        "            if \"yes\" in result:\n",
        "                return 1, result\n",
        "            elif \"no\" in result:\n",
        "                return 0, result\n",
        "            else:\n",
        "                return 2, result  # uncertain\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\"Error occurred:\", e)\n",
        "            tries += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "    return \"Error\", \"\""
      ],
      "metadata": {
        "id": "BCkZKCniHiLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"PRAGMA table_info(articles_data);\")\n",
        "columns = cursor.fetchall()\n",
        "col_names = [col[1] for col in columns]\n",
        "print(\"Columns in articles_data:\", col_names)\n",
        "\n",
        "cursor.execute(\"SELECT* FROM articles_data;\")\n",
        "rows = cursor.fetchall()\n",
        "print(rows)\n",
        "# Close the connection to the database\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "QTCzOFdUHjrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute(\"SELECT id, pmid, doi, title, abstract FROM articles_data\")\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "not_related_pmids = []  # store PMIDs where is_related = 0\n",
        "\n",
        "for row in rows:\n",
        "    id, pmid, doi, title, abstract = row\n",
        "\n",
        "    prompt_temp = prompt_1.replace(\"[Insert Paper Title Here]\", title or \"\")\n",
        "    prompt_temp = prompt_temp.replace(\"[Insert Paper Abstract Here]\", abstract or \"\")\n",
        "\n",
        "    isRelevant, res = isRelevantPaper(prompt_temp)\n",
        "\n",
        "    cursor.execute(\"UPDATE articles_data SET is_related=? WHERE id=?\", (isRelevant, id))\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"PMID {pmid}: is_related={isRelevant}\")\n",
        "\n",
        "conn.close()\n",
        "\n",
        "# Double check the not related articles manually\n",
        "if not_related_pmids:\n",
        "    print(\"\\nPMIDs marked as NOT related:\")\n",
        "    for pmid in not_related_pmids:\n",
        "        print(pmid)"
      ],
      "metadata": {
        "id": "Z7XQdh0gHk1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "\n",
        "# Load the entire articles_data table into a DataFrame\n",
        "df = pd.read_sql_query(\"SELECT * FROM articles_data\", conn)\n",
        "\n",
        "# Save to Excel\n",
        "output_path = str(OUTPUT_DIR / \"articles_data_list.xlsx\")\n",
        "df.to_excel(output_path, index=False)\n",
        "\n",
        "conn.close()\n",
        "\n",
        "print(\"Exported articles_data to:\", output_path)"
      ],
      "metadata": {
        "id": "hlH3cCaLHmqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts used for data extraction"
      ],
      "metadata": {
        "id": "qNG-UECUHoLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1. Read the txt file\n",
        "with open(str(DATA_DIR / \"organs_tissues_list.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "# 2. Parse the string into a Python dictionary\n",
        "organs_dict = json.loads(data)\n",
        "\n",
        "# 3. Save as JSON\n",
        "with open(str(DATA_DIR / \"organs_tissues_list.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(organs_dict, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Converted to organs_tissues_list.json\")"
      ],
      "metadata": {
        "id": "4iB1K9GrHpPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# 1. Read the txt file\n",
        "with open(str(DATA_DIR / \"species_list.txt\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "# 2. Parse the string into a Python dictionary\n",
        "organs_dict = json.loads(data)\n",
        "\n",
        "# 3. Save as JSON\n",
        "with open(str(DATA_DIR / \"species_list.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(organs_dict, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Converted to species_list.json\")"
      ],
      "metadata": {
        "id": "k07up0HPHuwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "# Load your PFAS list\n",
        "pfas = json.loads(Path(str(DATA_DIR / \"pfas_search_terms.json\")).read_text(encoding=\"utf-8\"))\n",
        "\n",
        "def make_target_chem_block(pfas_dict, max_syn=6):\n",
        "    \"\"\"\n",
        "    Build a block of canonical PFAS names with a few synonyms for prompt context\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    for canon, syns in pfas_dict.items():\n",
        "        syns_clean = [str(s).strip() for s in syns if str(s).strip()]\n",
        "        line = f\"{canon} → \" + \"; \".join(syns_clean[:max_syn])\n",
        "        lines.append(line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "target_chem_block = make_target_chem_block(pfas)"
      ],
      "metadata": {
        "id": "Uya2a3mjHxN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Paths to your JSON files\n",
        "species_path = str(DATA_DIR / \"species_list.json\")\n",
        "organs_path = str(DATA_DIR / \"organs_tissues_list.json\")\n",
        "\n",
        "# Load them into Python dictionaries\n",
        "with open(species_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    species_dict = json.load(f)\n",
        "\n",
        "with open(organs_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    organs_dict = json.load(f)\n",
        "\n",
        "print(\"JSONs loaded\")\n",
        "print(\"Species categories:\", species_dict.keys())\n",
        "print(\"Organs categories:\", organs_dict.keys())"
      ],
      "metadata": {
        "id": "zyTk5JvlH05N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_main_data_extraction_v2 = \"\"\"Task: Read the research article text and extract the requested items in the exact schema below.\n",
        "\n",
        "Article Text:\n",
        "{article_text}\n",
        "\n",
        "Important constraints (read carefully):\n",
        "• Chemicals: Report ONLY chemicals that appear in the “Target PFAS list” provided below (or any of their listed synonyms/CAS/SMILES). If none match, write “No target PFAS mentioned”.\n",
        "• Canonical names: When a match occurs, output the chemical’s CANONICAL name exactly as shown in the list, not the synonym found in the text. Optionally include the matched synonym in parentheses.\n",
        "• Keep answers concise and factual; do not infer.\n",
        "\n",
        "Target PFAS list (canonical → synonyms & identifiers):\n",
        "{target_chem_block}\n",
        "\n",
        "—— OUTPUT SCHEMA ——\n",
        "General Extraction\n",
        "- Chemicals:\n",
        "  Canonical PFAS names only (see above rule)\n",
        "- Species:\n",
        "  Only extract species names from the provided canonical list:\n",
        "  {list(species_dict.keys())}\n",
        "- Plasma Consideration:\n",
        "  [Yes/No]\n",
        "- Organs and Tissues Involved:\n",
        "  Only extract from the provided canonical list:\n",
        "  {list(organs_dict.keys())}\n",
        "- Research Outcome:\n",
        "  [1–3 sentences; include numeric metrics if present]\n",
        "- Type of Study:\n",
        "  [Human/Animal/In-vitro]\n",
        "\n",
        "If Type of Study = Human, also provide:\n",
        "- Study Type:\n",
        "- Location:\n",
        "- Gender of Subjects:\n",
        "- Age of Subjects:\n",
        "- Number of Subjects:\n",
        "- Experimental Samples Involved:\n",
        "\n",
        "If Type of Study = Animal, also provide:\n",
        "- Study Type: [Experimental/Observational, species names only]\n",
        "- Gender of Subjects:\n",
        "- Age at Exposure:\n",
        "- Route of Exposure:\n",
        "\n",
        "Formatting rules:\n",
        "• Use the headings exactly as above, in the same order.\n",
        "• If an item is not reported, write “Not stated”.\n",
        "• Do not include any other commentary.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "f_FcidPzH5e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction"
      ],
      "metadata": {
        "id": "M9i_iWLLH9Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_encodings = tiktoken.list_encoding_names()\n",
        "print(available_encodings)"
      ],
      "metadata": {
        "id": "S6rXGNKAH7Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(input_string, model):\n",
        "    # Initialize the tokenizer for the GPT-4 model\n",
        "    encoding = tiktoken.encoding_for_model(model)\n",
        "\n",
        "    # Encode the input string into tokens\n",
        "    tokens = encoding.encode(input_string)\n",
        "\n",
        "    # Return the number of tokens\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "LnvLYtIBH_9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "query = '''\n",
        "    SELECT * from articles_blob_data\n",
        "'''\n",
        "\n",
        "# Optionally, retrieve and display the blob data to verify\n",
        "cursor.execute(query)\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "conn.close()  # Close the connection"
      ],
      "metadata": {
        "id": "QPwG2s45IDF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMID_unprocessed = {'417812','2906022', '3684504','6759044',  '6819698', '7299662', '7337230', '7639356', '7849929', '7884142', '7974521', '8229349', '8250967', '8516773', '9269454', '9862284'} # Randomly select one PMID to test prompt_main_data_extraction_V2"
      ],
      "metadata": {
        "id": "0Hfxw1bBIIQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMID_unprocessed = {'11519538', '11719891', '11855757', '15328768', '15366585', '16466536', '20556880', '2093123', '2374085', '25454233', '30411895', '30528102', '30631142', '31549993', '31568513', '32495786', '32897586', '33017053', '33382826', '33605484', '33647664', '34854961', '35138827', '35324171', '35580034', '37220906', '37984148', '38801906', '39504592', '39542374', '39556161', '40239480', '417812'}"
      ],
      "metadata": {
        "id": "NHi21sfuIJPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMID_unprocessed = {'2781142', '2901469', '3089945', '3098413', '3140922', '3179494', '3575876', '3759552', '6470567', '6515128', '6547941', '6787865',\n",
        "                    '7153234', '7784553', '7849907', '7849908', '7849919', '7849920', '7978424', '8047981', '8079710', '8079711', '8097775', '8134252',\n",
        "                    '8158684', '8333387', '8336905', '8429782', '8482678', '8611942', '8632927', '8646338', '8795104', '8870975', '8890829', '9084907',\n",
        "                    '9177987', '9185039', '9219835', '9341714', '9383597', '9419473', '9625556', '9655737'}"
      ],
      "metadata": {
        "id": "EVDGUo8gIK0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Automatically fetch all PMIDs where is_related = 1\n",
        "cursor.execute(\"SELECT pmid FROM articles_data WHERE is_related = 1\")\n",
        "pmid_rows = cursor.fetchall()\n",
        "\n",
        "# Flatten into a Python list of strings\n",
        "PMID_unprocessed = [str(row[0]) for row in pmid_rows]\n",
        "\n",
        "print(\"Fetched PMIDs:\", PMID_unprocessed)\n",
        "print(\"Total count:\", len(PMID_unprocessed))\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "fpVBlXUMIMIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PMID_unprocessed = [str(p) for p in PMID_unprocessed]\n",
        "\n",
        "# Find the position of the cutoff PMID\n",
        "cutoff = \"32790152\"\n",
        "if cutoff in PMID_unprocessed:\n",
        "    cutoff_index = PMID_unprocessed.index(cutoff)\n",
        "    PMID_unprocessed = PMID_unprocessed[cutoff_index + 1:]  # exclude cutoff itself\n",
        "else:\n",
        "    print(f\"{cutoff} not found in PMID_unprocessed\")\n",
        "\n",
        "print(\"New total count:\", len(PMID_unprocessed))\n",
        "print(\"First 10 PMIDs after cutoff:\", PMID_unprocessed[:10])"
      ],
      "metadata": {
        "id": "zkxWfxHRIQOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "input_formating = {}\n",
        "\n",
        "# Convert the set to a tuple for use in the SQL query\n",
        "pmid_tuple = tuple(PMID_unprocessed)\n",
        "\n",
        "# Query to retrieve matching rows from articles_blob_data\n",
        "query = '''\n",
        "    SELECT * FROM articles_blob_data WHERE PMID IN ({placeholders})\n",
        "'''.format(placeholders=','.join(['?'] * len(pmid_tuple)))\n",
        "\n",
        "# Execute the query with the DOIs as parameters\n",
        "cursor.execute(query, pmid_tuple)\n",
        "pmid_unprocessed_rows = cursor.fetchall()\n",
        "\n",
        "for pmid_unprocessed_row in pmid_unprocessed_rows:\n",
        "    pmid = pmid_unprocessed_row[0]  # Assuming pmid is in the first column\n",
        "    body = pmid_unprocessed_row[3]  # Assuming body is in the third column\n",
        "\n",
        "    cursor.execute('SELECT COUNT(*) FROM articles_data WHERE pmid = ?', (pmid,))\n",
        "    present_pmid_count = cursor.fetchone()[0]\n",
        "    # If PMID is not present, insert the data\n",
        "    if present_pmid_count == 0:\n",
        "        print(\"Not in relavent document\")\n",
        "        print(body[:10])  # Display the body\n",
        "\n",
        "        # cursor.execute(\n",
        "        #     '''\n",
        "        #     INSERT INTO relavent_articles_data (doi, body)\n",
        "        #     VALUES (?, ?)\n",
        "        #     ''', (doi, body)\n",
        "        # )\n",
        "        # conn.commit()  # Commit the insert operation\n",
        "\n",
        "conn.close()  # Close the connection"
      ],
      "metadata": {
        "id": "6bD0Kd7nIQ2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_preview = pd.DataFrame(pmid_unprocessed_rows)\n",
        "print(df_preview.head())"
      ],
      "metadata": {
        "id": "eoZpqxBZIVrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect(str(DB_PATH))\n",
        "cursor = conn.cursor()\n",
        "\n",
        "input_formating = {}\n",
        "\n",
        "for i,row in enumerate(pmid_unprocessed_rows):\n",
        "    pmid = row[0]  # Assuming pmid is in the first column\n",
        "    article_text = row[3]\n",
        "\n",
        "    #print(article_text)\n",
        "\n",
        "    # Check if article_text is bytes, then decode it; otherwise, it's assumed to be a string\n",
        "    if isinstance(article_text, bytes):\n",
        "        article_text = article_text.decode('utf-8')\n",
        "\n",
        "    sentences = article_text.split(\". \")\n",
        "    context_length = 128000\n",
        "\n",
        "    # Calculate token counts for each sentence\n",
        "    tokens_count = count_tokens(article_text, \"gpt-oss-120b\")\n",
        "    if(tokens_count>context_length):\n",
        "        partitions = math.ceil(tokens_count/context_length)\n",
        "        print(i, tokens_count, len(article_text.split()), row[1], \"---------------------------------------------------------\", partitions)\n",
        "        approx_partition_size = math.ceil(len(article_text) / partitions)\n",
        "        input_formating[pmid]=[]\n",
        "        start = 0\n",
        "        for k in range(partitions):\n",
        "            # remaining_text = article_text[start:]\n",
        "            if k == partitions - 1:\n",
        "                # For the last partition, take the remaining text\n",
        "                end = len(article_text)\n",
        "            else:\n",
        "                # Find the approximate end position\n",
        "                end = start + approx_partition_size\n",
        "\n",
        "                # Find the nearest line break to the desired partition size\n",
        "                # end = article_text.rfind('.', start, end)\n",
        "                if end == -1:  # If no line break is found, use the approx size\n",
        "                    end = start + approx_partition_size\n",
        "\n",
        "\n",
        "            prompt_main_data_extraction_v2_temp = prompt_main_data_extraction_v2\n",
        "            input_formating[row[1]].append(prompt_main_data_extraction_v2_temp.replace(\"{article_text}\",article_text[start: end]))\n",
        "            start = end + 1\n",
        "    else:\n",
        "        print(i, tokens_count, len(article_text.split()), row[1])\n",
        "\n",
        "        prompt_main_data_extraction_v2_temp = prompt_main_data_extraction_v2\n",
        "        input_formating[pmid]=[prompt_main_data_extraction_v2_temp.replace(\"{article_text}\",article_text)]\n",
        "        print(pmid,'|',article_text)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "g5YyabBfIWAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _canonical_block_from_dict_keys(d):\n",
        "    # d can be a nested dict (e.g., categories -> items) or a flat dict\n",
        "    # We only list canonical names (keys if flat, items if nested)\n",
        "    names = []\n",
        "    for k, v in d.items():\n",
        "        # If value is a list of names inside a category, include those names\n",
        "        if isinstance(v, (list, tuple, set)):\n",
        "            names.extend([str(x).strip() for x in v if str(x).strip()])\n",
        "        else:\n",
        "            # Treat top-level keys as canonical names\n",
        "            names.append(str(k).strip())\n",
        "    # unique, sorted\n",
        "    names = sorted({n for n in names if n})\n",
        "    # pretty list (one per line with dash)\n",
        "    return \"\\n  - \" + \"\\n  - \".join(names) if names else \"\\n  - (none)\""
      ],
      "metadata": {
        "id": "WTkxrvKmIdEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_extraction_prompt(article_text, target_chem_block, species_dict, organs_dict):\n",
        "    species_list = _canonical_block_from_dict_keys(species_dict)\n",
        "    organs_list  = _canonical_block_from_dict_keys(organs_dict)\n",
        "\n",
        "    return prompt_main_data_extraction_v2.format(\n",
        "        article_text      = article_text,\n",
        "        target_chem_block = target_chem_block,\n",
        "        species_list      = species_list,\n",
        "        organs_list       = organs_list,\n",
        "    )"
      ],
      "metadata": {
        "id": "aJ6Xb0ndIdoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from openai import OpenAI, RateLimitError, APIConnectionError, APIError\n",
        "\n",
        "client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL if LLM_BASE_URL else None)\n",
        "\n",
        "def outputValuesExtractor(prompt_main, model=LLM_MODEL, temperature=0):\n",
        "    \"\"\"\n",
        "    Call the model once and return the text. Uses the new OpenAI SDK (v1).\n",
        "    Retries a few times on transient errors. Returns 'ErrorInPFASTK: ...' on failure.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\",\n",
        "         \"content\": \"You are an assistant trained to extract specific information from research articles related to pharmacokinetics and toxicokinetics.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt_main},\n",
        "    ]\n",
        "\n",
        "    max_retries = 4\n",
        "    backoff = 2.0\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            resp = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                temperature=temperature,\n",
        "                # do NOT set max_tokens to a negative or zero; omitting is fine\n",
        "            )\n",
        "            return (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "        except (RateLimitError, APIConnectionError, APIError) as e:\n",
        "            # transient API errors — backoff and retry\n",
        "            if attempt == max_retries - 1:\n",
        "                return f\"ErrorInPFASTK: {e}\"\n",
        "            time.sleep(backoff)\n",
        "            backoff *= 1.5\n",
        "\n",
        "        except Exception as e:\n",
        "            # anything else: stop and return the error\n",
        "            return f\"ErrorInPFASTK: {e}\""
      ],
      "metadata": {
        "id": "Fxv-amMYIfCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "OUTPUT_SEP = \"\\n<output-seperator>\\n\"\n",
        "\n",
        "def build_result_table_simple(input_formating, extractor_func):\n",
        "    rows = []\n",
        "    pmids = list(input_formating.keys())\n",
        "\n",
        "    for pmid in tqdm(pmids, desc=\"Extracting\"):\n",
        "        prompts = input_formating.get(pmid, [])\n",
        "        if not prompts:\n",
        "            rows.append({\n",
        "                \"pmid\": pmid,\n",
        "                \"output\": \"\",\n",
        "                \"combined_output\": \"\",\n",
        "                \"chemicals\": None,\n",
        "                \"species\": None,\n",
        "                \"plasma_consideration\": None,\n",
        "                \"organs\": None,\n",
        "                \"research_outcome\": None,\n",
        "                \"type_of_study\": None,\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        part_outputs = []\n",
        "        for p in prompts:\n",
        "            try:\n",
        "                res = extractor_func(p)\n",
        "            except Exception as e:\n",
        "                res = f\"ErrorInPFASTK: {e}\"\n",
        "            part_outputs.append(res if res else \"\")\n",
        "\n",
        "        first_output = part_outputs[0] if part_outputs else \"\"\n",
        "        combined_output = OUTPUT_SEP.join(part_outputs)\n",
        "\n",
        "        rows.append({\n",
        "            \"pmid\": pmid,\n",
        "            \"output\": first_output,\n",
        "            \"combined_output\": combined_output,\n",
        "            \"chemicals\": None,\n",
        "            \"species\": None,\n",
        "            \"plasma_consideration\": None,\n",
        "            \"organs\": None,\n",
        "            \"research_outcome\": None,\n",
        "            \"type_of_study\": None,\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=[\n",
        "        \"pmid\", \"output\", \"combined_output\",\n",
        "        \"chemicals\", \"species\", \"plasma_consideration\", \"organs\",\n",
        "        \"research_outcome\", \"type_of_study\"\n",
        "    ])\n",
        "\n",
        "\n",
        "    display(df.head(5))\n",
        "    return df"
      ],
      "metadata": {
        "id": "vxXF842HIlZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = build_result_table_simple(input_formating, outputValuesExtractor)"
      ],
      "metadata": {
        "id": "AMsLpd2WIorF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def _clean_list(text):\n",
        "    if not text:\n",
        "        return None\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'^\\[|\\]$', '', text).strip()   # strip surrounding brackets if present\n",
        "    parts = [p.strip(\" -•\\t\\r\\n,\") for p in re.split(r'[,;\\n]+', text) if p.strip()]\n",
        "    return parts or None\n",
        "\n",
        "def _norm(s):\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.replace(\"**\", \"\")                # drop markdown bold\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", s)          # normalize spaces\n",
        "    return s\n",
        "\n",
        "# Headings WITHOUT the trailing colon; we'll accept either \"Header\" or \"Header:\"\n",
        "HEADERS = [\n",
        "    \"chemicals\",\n",
        "    \"species\",\n",
        "    \"plasma consideration\",\n",
        "    \"organs and tissues involved\",\n",
        "    \"research outcome\",\n",
        "    \"type of study\",\n",
        "    \"gender of subjects\",\n",
        "    \"age of subjects\",\n",
        "    \"route of exposure\",\n",
        "    \"number_of_subjects\",\n",
        "    \"experimental_samples_involved\"\n",
        "]\n",
        "\n",
        "def parse_combined_output(text):\n",
        "    \"\"\"\n",
        "    Parse 'combined_output' (or 'output') into fields, tolerating headers\n",
        "    with or without a trailing colon.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return { \"chemicals\": None, \"species\": None, \"plasma_consideration\": None,\n",
        "                 \"organs\": None, \"research_outcome\": None, \"type_of_study\": None,\n",
        "                 \"gender_of_subjects\": None, \"age_at_exposure\": None, \"route_of_exposure\": None,\n",
        "                 \"number_of_subjects\": None, \"experimental_samples_involved\": None}\n",
        "\n",
        "    # Normalize and lowercase working copy (same length as original t_raw).\n",
        "    t_raw = _norm(text)\n",
        "    t_lc  = t_raw.lower()\n",
        "\n",
        "    # Find header spans (start & end of the header token itself)\n",
        "    spans = []\n",
        "    for h in HEADERS:\n",
        "        # match e.g. \"research outcome\" or \"research outcome:\" (case-insensitive)\n",
        "        pat = rf\"(?s)\\b{re.escape(h)}\\b\\s*:?\"\n",
        "        m = re.search(pat, t_lc)\n",
        "        if m:\n",
        "            spans.append((h, m.start(), m.end()))\n",
        "\n",
        "    if not spans:\n",
        "        return { \"chemicals\": None, \"species\": None, \"plasma_consideration\": None,\n",
        "                 \"organs\": None, \"research_outcome\": None, \"type_of_study\": None,\n",
        "                 \"gender_of_subjects\": None, \"age_at_exposure\": None, \"route_of_exposure\": None,\n",
        "                 \"number_of_subjects\": None, \"experimental_samples_involved\": None}\n",
        "\n",
        "    # Sort headers by position\n",
        "    spans.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Build mapping header -> content between it and the next header\n",
        "    sections = {}\n",
        "    for i, (h, start, hdr_end) in enumerate(spans):\n",
        "        next_start = spans[i+1][1] if i+1 < len(spans) else len(t_raw)\n",
        "        chunk = t_raw[hdr_end:next_start].strip()\n",
        "        sections[h] = chunk\n",
        "\n",
        "    # Extract and clean fields\n",
        "    chemicals            = _clean_list(sections.get(\"chemicals\"))\n",
        "    species              = _clean_list(sections.get(\"species\"))\n",
        "    plasma_consideration = sections.get(\"plasma consideration\")\n",
        "    if plasma_consideration:\n",
        "        plasma_consideration = plasma_consideration.splitlines()[0].strip()\n",
        "\n",
        "    organs               = _clean_list(sections.get(\"organs and tissues involved\"))\n",
        "\n",
        "    research_outcome     = sections.get(\"research outcome\")\n",
        "    if research_outcome:\n",
        "        research_outcome = re.sub(r\"[ \\t]+\", \" \", research_outcome).strip()\n",
        "\n",
        "    type_of_study        = sections.get(\"type of study\")\n",
        "    if type_of_study:\n",
        "        type_of_study = re.sub(r'^\\[|\\]$', '', type_of_study.splitlines()[0]).strip()\n",
        "\n",
        "    gender = sections.get(\"gender of subjects\")\n",
        "    if gender:\n",
        "        gender = re.sub(r\"\\*\\*|_\", \"\", gender.strip())\n",
        "\n",
        "    age = sections.get(\"age at exposure\")\n",
        "    if age:\n",
        "        age = re.sub(r\"\\*\\*|_\", \"\", age.strip())\n",
        "\n",
        "    route = sections.get(\"route of exposure\")\n",
        "    if route:\n",
        "        route = re.sub(r\"\\*\\*|_\", \"\", route.strip())\n",
        "\n",
        "    subject = sections.get(\"number of subjects\")\n",
        "    if subject:\n",
        "        subject = re.sub(r\"\\*\\*|_\", \"\", subject.strip())\n",
        "\n",
        "    sample = sections.get(\"experimental samples involved\")\n",
        "    if sample:\n",
        "        sample = re.sub(r\"\\*\\*|_\", \"\", samples.strip())\n",
        "\n",
        "    return {\n",
        "        \"chemicals\": chemicals,\n",
        "        \"species\": species,\n",
        "        \"plasma_consideration\": plasma_consideration,\n",
        "        \"organs\": organs,\n",
        "        \"research_outcome\": research_outcome,\n",
        "        \"type_of_study\": type_of_study,\n",
        "        \"gender_of_subjects\": gender,\n",
        "        \"age_at_exposure\": age,\n",
        "        \"route_of_exposure\": route,\n",
        "        \"number_of_subjects\": subject,\n",
        "        \"experimental_samples_involved\": sample\n",
        "    }\n",
        "\n",
        "# --- Apply to your dataframe again ---\n",
        "source_text = df_result[\"combined_output\"].fillna(df_result[\"output\"])\n",
        "parsed = source_text.apply(parse_combined_output)\n",
        "parsed_df = pd.DataFrame(parsed.tolist())\n",
        "\n",
        "# For display, join list-like fields\n",
        "for col in [\"chemicals\", \"species\", \"organs\"]:\n",
        "    parsed_df[col] = parsed_df[col].apply(lambda x: \"; \".join(x) if isinstance(x, list) else x)\n",
        "\n",
        "df_result[[\"chemicals\", \"species\", \"plasma_consideration\",\n",
        "           \"organs\", \"research_outcome\", \"type_of_study\",\"gender_of_subjects\", \"age_at_exposure\", \"route_of_exposure\", \"number_of_subjects\", \"experimental_samples_involved\"]] = parsed_df[\n",
        "               [\"chemicals\",\"species\",\"plasma_consideration\",\"organs\",\"research_outcome\",\"type_of_study\", \"gender_of_subjects\", \"age_at_exposure\", \"route_of_exposure\", \"number_of_subjects\", \"experimental_samples_involved\"]\n",
        "           ]\n",
        "\n",
        "display(df_result)"
      ],
      "metadata": {
        "id": "Xzsn1dJnIq29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv(str(OUTPUT_DIR / \"df_result_last.csv\"), index=False, encoding = \"utf-8\")"
      ],
      "metadata": {
        "id": "F6YMu1MsIxIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Token_lengths = []\n",
        "for input_formats in input_formating:\n",
        "    single_token_count=0\n",
        "    for prompts in input_formating[input_formats]:\n",
        "        single_token_count+=count_tokens(prompts, \"gpt-4-turbo\")\n",
        "    Token_lengths.append(single_token_count)\n",
        "\n",
        "TokenCountsMillions=sum(Token_lengths)/1000000\n",
        "print(str(TokenCountsMillions) + \" Million Tokens = \"+str((TokenCountsMillions)*5)+\" $\")"
      ],
      "metadata": {
        "id": "fCvEcHBdIyHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figure"
      ],
      "metadata": {
        "id": "eG5tF73fyVNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "KhADks-hyXXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op8Lisv6yR8R"
      },
      "outputs": [],
      "source": [
        "!pip install plotly -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pubchempy"
      ],
      "metadata": {
        "id": "xOC3jNhlyZ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pubchempy ftfy -q"
      ],
      "metadata": {
        "id": "1dMsNU25ya_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from matplotlib.colors import BoundaryNorm\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as colors"
      ],
      "metadata": {
        "id": "f98Joqv7yb80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "RDNxaS30ydWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset corresponding to Table S2\n",
        "df = pd.read_csv(\"table_s2.csv\")"
      ],
      "metadata": {
        "id": "Vi6LvmTiydr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "ZLM8ruwgyfoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset corresponding to Table S1\n",
        "PFAS430 = pd.read_csv(\"table_S1.csv\")"
      ],
      "metadata": {
        "id": "DGkkmdpXygfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting"
      ],
      "metadata": {
        "id": "QpGuBTwvyhQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.colors import LinearSegmentedColormap, BoundaryNorm\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "CSV_PATH = \"/content/pfas_dataset.csv\"   # file path\n",
        "CHEM_COL = \"chemicals\"                   # as provided\n",
        "SPECIES_COL = \"species\"                  # as provided\n",
        "ORGANS_COL = \"organs\"                    # as provided\n",
        "PMID_COL = \"pmid\"                        # used to count \"records\"\n",
        "\n",
        "TOPN = 20\n",
        "SAVE_DPI = 600\n",
        "SAVE_KW = dict(format=\"tiff\", dpi=SAVE_DPI, pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.dpi\": 110,\n",
        "    \"savefig.dpi\": SAVE_DPI,\n",
        "    \"font.size\": 12,          # base font\n",
        "    \"axes.titlesize\": 20,     # title\n",
        "    \"axes.labelsize\": 16,     # x/y labels\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "})\n",
        "\n",
        "# =========================\n",
        "# SPECIES_NORMALIZE\n",
        "# =========================\n",
        "SPECIES_NORMALIZE = {\n",
        "    # Humans & Primates\n",
        "    \"human\": \"Human\", \"humans\": \"Human\",\n",
        "    \"nonhuman primate\": \"Nonhuman primate\", \"non human primate\": \"Nonhuman primate\",\n",
        "    \"monkey\": \"Monkey\", \"monkeys\": \"Monkey\",\n",
        "\n",
        "    # Rodents / Lab Animals\n",
        "    \"rat\": \"Rat\", \"rats\": \"Rat\",\n",
        "    \"mouse\": \"Mouse\", \"mice\": \"Mouse\", \"mice (mus musculus)\": \"Mouse\",\n",
        "    \"hamster\": \"Hamster\", \"hamsters\": \"Hamster\",\n",
        "    \"rabbit\": \"Rabbit\", \"rabbits\": \"Rabbit\",\n",
        "\n",
        "    # Domestic Mammals / Livestock\n",
        "    \"dog\": \"Dog\", \"dogs\": \"Dog\",\n",
        "    \"cat\": \"Cat\", \"cats\": \"Cat\",\n",
        "    \"pig\": \"Pig\", \"pigs\": \"Pig\", \"porcine\": \"Pig\", \"porcine (pig)\": \"Pig\",\n",
        "    \"cattle\": \"Cattle\", \"beef cattle\": \"Cattle\", \"dairy cows\": \"Cattle\", \"dairy cattle\": \"Cattle\", \"bovine\": \"Cattle\",\n",
        "    \"sheep\": \"Sheep\", \"ram\": \"Sheep\",\n",
        "    \"goat\": \"Goat\", \"goats\": \"Goat\",\n",
        "    \"horse\": \"Horse\", \"horses\": \"Horse\",\n",
        "    \"camel\": \"Camel\",\n",
        "\n",
        "    # Poultry / Birds\n",
        "    \"chicken\": \"Chicken\", \"chickens\": \"Chicken\", \"broiler\": \"Chicken\",\n",
        "    \"duck\": \"Duck\", \"ducks\": \"Duck\",\n",
        "    \"turkey\": \"Turkey\",\n",
        "    \"quail\": \"Quail\",\n",
        "    \"bird\": \"Bird\", \"birds\": \"Bird\",\n",
        "\n",
        "    # Fish (General & Specific)\n",
        "    \"fish\": \"Fish\", \"fishes\": \"Fish\",\n",
        "    \"zebrafish\": \"Zebrafish\", \"zebra fish\": \"Zebrafish\",\n",
        "    \"carp\": \"Carp\", \"common carp\": \"Carp\", \"crucian carp\": \"Carp\", \"grass carp\": \"Carp\", \"bighead carp\": \"Carp\",\n",
        "    \"catfish\": \"Catfish\", \"yellow cat fish\": \"Catfish\",\n",
        "    \"tilapia\": \"Tilapia\",\n",
        "    \"minnow\": \"Minnow\",\n",
        "    \"goby\": \"Goby\",\n",
        "    \"mudskipper\": \"Mudskipper\",\n",
        "    \"mullet\": \"Mullet\",\n",
        "    \"shad\": \"Shad\",\n",
        "    \"rockfish\": \"Rockfish\",\n",
        "    \"shark\": \"Shark\", \"sharks\": \"Shark\",\n",
        "    \"snakehead\": \"Snakehead\",\n",
        "    \"midge\": \"Midge\", # Often aquatic larvae, grouping near fish/aquatic context or keep separate\n",
        "\n",
        "    # Marine Mammals & Large Wildlife\n",
        "    \"dolphin\": \"Dolphin\",\n",
        "    \"porpoise\": \"Porpoise\",\n",
        "    \"whale\": \"Whale\", \"beluga whale\": \"Whale\",\n",
        "    \"seal\": \"Seal\", \"ringed seal\": \"Seal\",\n",
        "    \"sea lion\": \"Sea lion\",\n",
        "    \"sea otter\": \"Sea otter\",\n",
        "    \"polar bear\": \"Polar bear\", \"polar bears\": \"Polar bear\",\n",
        "    \"walrus\": \"Walrus\",\n",
        "    \"otter\": \"Otter\",\n",
        "    \"fox\": \"Fox\",\n",
        "    \"mink\": \"Mink\",\n",
        "    \"caribou\": \"Caribou\",\n",
        "    \"wolf\": \"Wolf\",\n",
        "    \"tiger\": \"Tiger\",\n",
        "\n",
        "    # Invertebrates (Aquatic & Terrestrial)\n",
        "    \"mussel\": \"Mussel\", \"mussels\": \"Mussel\",\n",
        "    \"oyster\": \"Oyster\", \"oysters\": \"Oyster\",\n",
        "    \"clam\": \"Clam\", \"clams\": \"Clam\",\n",
        "    \"bivalve\": \"Bivalve\", \"bivalves\": \"Bivalve\", \"bivalvia\": \"Bivalve\",\n",
        "    \"crustacean\": \"Crustacean\", \"crustaceans\": \"Crustacean\",\n",
        "    \"gastropod\": \"Gastropod\", \"gastropods\": \"Gastropod\",\n",
        "    \"cephalopod\": \"Cephalopod\", \"cephalopods\": \"Cephalopod\",\n",
        "    \"shrimp\": \"Shrimp\", \"prawn\": \"Shrimp\",\n",
        "    \"crab\": \"Crab\", \"blue crab\": \"Crab\",\n",
        "    \"lobster\": \"Lobster\",\n",
        "    \"crayfish\": \"Crayfish\",\n",
        "    \"gammarid\": \"Gammarid\",\n",
        "    \"mysid\": \"Mysid\",\n",
        "    \"amphipod\": \"Amphipod\",\n",
        "    \"diporeia\": \"Amphipod\", # Specific amphipod\n",
        "    \"zooplankton\": \"Zooplankton\",\n",
        "    \"plankton\": \"Plankton\",\n",
        "    \"bentho\": \"Benthos\",\n",
        "    \"lugworm\": \"Worm\",\n",
        "    \"worm\": \"Worm\",\n",
        "    \"earthworm\": \"Earthworm\",\n",
        "    \"snail\": \"Snail\",\n",
        "    \"whelk\": \"Whelk\",\n",
        "    \"limpet\": \"Limpet\",\n",
        "    \"conch\": \"Conch\",\n",
        "    \"abalone\": \"Abalone\",\n",
        "    \"urchin\": \"Sea urchin\",\n",
        "    \"sea star\": \"Starfish\",\n",
        "    \"anemone\": \"Sea anemone\",\n",
        "    \"sea squirt\": \"Sea squirt\",\n",
        "    \"isopod\": \"Isopod\",\n",
        "    \"cricket\": \"Cricket\",\n",
        "    \"honeybee\": \"Honeybee\",\n",
        "\n",
        "    # Amphibians & Reptiles\n",
        "    \"frog\": \"Frog\", \"frogs\": \"Frog\", \"bullfrog\": \"Frog\", \"tadpole\": \"Frog\",\n",
        "    \"amphibian\": \"Amphibian\",\n",
        "    \"snake\": \"Snake\",\n",
        "\n",
        "    # Other / Micro\n",
        "    \"bat\": \"Bat\",\n",
        "    \"nematode\": \"Nematode\",\n",
        "    \"bacteria\": \"Bacteria\",\n",
        "    \"algae\": \"Algae\",\n",
        "    \"invertebrates\": \"Invertebrates\",\n",
        "}\n",
        "\n",
        "# =========================\n",
        "# ORGANS_NORMALIZE\n",
        "# =========================\n",
        "\n",
        "ORGANS_NORMALIZE = {\n",
        "    # Core Organs\n",
        "    \"liver\": \"Liver\", \"livers\": \"Liver\", \"hepatocytes\": \"Liver\", \"hepatopancreas\": \"Liver\", # Hepatopancreas functionally similar in inverts\n",
        "    \"kidney\": \"Kidney\", \"kidneys\": \"Kidney\",\n",
        "    \"brain\": \"Brain\", \"cortex\": \"Brain\", \"hippocampus\": \"Brain\", \"cerebellum\": \"Brain\", \"cerebral cortex\": \"Brain\",\n",
        "    \"lung\": \"Lung\", \"lungs\": \"Lung\",\n",
        "    \"heart\": \"Heart\", \"heart muscle\": \"Heart\",\n",
        "    \"spleen\": \"Spleen\",\n",
        "    \"pancreas\": \"Pancreas\",\n",
        "    \"gallbladder\": \"Gallbladder\", \"gall bladder\": \"Gallbladder\",\n",
        "\n",
        "    # Muscle & Flesh\n",
        "    \"muscle\": \"Muscle\", \"muscles\": \"Muscle\", \"skeletal muscle\": \"Muscle\",\n",
        "    \"gastrocnemius\": \"Muscle\", \"gastrocnemius muscle\": \"Muscle\", \"thigh\": \"Muscle\",\n",
        "    \"fillet\": \"Muscle\", # Common in fish\n",
        "    \"meat\": \"Muscle\",\n",
        "\n",
        "    # Aquatic Specific\n",
        "    \"gill\": \"Gills\", \"gills\": \"Gills\",\n",
        "    \"swim bladder\": \"Swim bladder\", \"swimming bladder\": \"Swim bladder\",\n",
        "    \"fin\": \"Fins\", \"fins\": \"Fins\",\n",
        "    \"scale\": \"Scales\",\n",
        "    \"mucus\": \"Mucus\",\n",
        "\n",
        "    # Gastrointestinal\n",
        "    \"intestine\": \"Intestine\", \"intestines\": \"Intestine\", \"small intestine\": \"Intestine\", \"large intestine\": \"Intestine\",\n",
        "    \"duodenum\": \"Intestine\", \"jejunum\": \"Intestine\", \"cecum\": \"Intestine\",\n",
        "    \"stomach\": \"Stomach\", \"gizzard\": \"Stomach\", \"digestive tract\": \"Gastrointestinal tract\", \"digestive system\": \"Gastrointestinal tract\",\n",
        "    \"gut\": \"Gastrointestinal tract\", \"gastrointestinal tract\": \"Gastrointestinal tract\",\n",
        "    \"gastric mucosa\": \"Stomach\",\n",
        "\n",
        "    # Blood & Matrices\n",
        "    \"blood\": \"Blood\", \"whole blood\": \"Whole blood\",\n",
        "    \"serum\": \"Serum\", \"blood serum\": \"Serum\", \"fetal serum\": \"Serum\", \"maternal serum\": \"Serum\", \"infant serum\": \"Serum\",\n",
        "    \"plasma\": \"Plasma\", \"blood plasma\": \"Plasma\", \"fetal plasma\": \"Plasma\", \"neonatal plasma\": \"Plasma\", \"arterial plasma\": \"Plasma\", \"venous plasma\": \"Plasma\",\n",
        "    \"cord blood\": \"Cord blood\",\n",
        "    \"red blood cells\": \"Red blood cells\", \"rbc\": \"Red blood cells\",\n",
        "    \"blood clot\": \"Blood\",\n",
        "    \"haemolymph\": \"Haemolymph\", # Invertebrate blood\n",
        "\n",
        "    # Excreta / Elimination\n",
        "    \"urine\": \"Urine\",\n",
        "    \"feces\": \"Feces\", \"faeces\": \"Feces\", \"stool\": \"Feces\",\n",
        "    \"bile\": \"Bile\", \"gallbladder bile\": \"Bile\",\n",
        "    \"sweat\": \"Sweat\",\n",
        "\n",
        "    # Endocrine / Reproductive\n",
        "    \"thyroid\": \"Thyroid\", \"thyroid gland\": \"Thyroid\",\n",
        "    \"adrenal\": \"Adrenal\", \"adrenals\": \"Adrenal\", \"adrenal gland\": \"Adrenal\", \"adrenal glands\": \"Adrenal\",\n",
        "    \"pituitary\": \"Pituitary\", \"pituitary gland\": \"Pituitary\",\n",
        "    \"testis\": \"Testis\", \"testes\": \"Testis\", \"leydig cells\": \"Testis\",\n",
        "    \"ovary\": \"Ovary\", \"ovaries\": \"Ovary\",\n",
        "    \"uterus\": \"Uterus\",\n",
        "    \"prostate\": \"Prostate\",\n",
        "    \"gonad\": \"Gonad\", \"gonads\": \"Gonad\", \"reproductive tract\": \"Gonad\",\n",
        "    \"mammary gland\": \"Mammary gland\", \"mammary tissue\": \"Mammary gland\", \"breast\": \"Mammary gland\", \"udder\": \"Mammary gland\",\n",
        "    \"cervix\": \"Cervix\",\n",
        "\n",
        "    # Development / Pregnancy\n",
        "    \"placenta\": \"Placenta\",\n",
        "    \"amniotic fluid\": \"Amniotic fluid\",\n",
        "    \"fetus\": \"Fetus\",\n",
        "    \"embryo\": \"Embryo\", \"whole embryo\": \"Embryo\",\n",
        "    \"yolk\": \"Egg yolk\", \"egg yolk\": \"Egg yolk\",\n",
        "    \"albumen\": \"Egg white\",\n",
        "    \"egg\": \"Egg\", \"eggs\": \"Egg\",\n",
        "\n",
        "    # Milk\n",
        "    \"milk\": \"Milk\", \"breast milk\": \"Breast milk\", \"breastmilk\": \"Breast milk\",\n",
        "\n",
        "    # Adipose / Skin / Bone / Integument\n",
        "    \"adipose\": \"Adipose tissue\", \"adipose tissue\": \"Adipose tissue\", \"fat\": \"Adipose tissue\", \"blubber\": \"Adipose tissue\",\n",
        "    \"white fat\": \"Adipose tissue\", \"white adipose tissue\": \"Adipose tissue\",\n",
        "    \"epididymal fat\": \"Adipose tissue\", \"abdominal fat\": \"Adipose tissue\", \"perigonadal adipose\": \"Adipose tissue\", \"subcutaneous fat\": \"Adipose tissue\",\n",
        "    \"fat pad\": \"Adipose tissue\", \"fat pads\": \"Adipose tissue\", \"inguinal fat pads\": \"Adipose tissue\",\n",
        "    \"lipid\": \"Adipose tissue\", # Context dependent, but usually implies fat extract\n",
        "    \"skin\": \"Skin\", \"integumenta\": \"Skin\",\n",
        "    \"bone\": \"Bone\", \"whole bone\": \"Bone\", \"femur\": \"Bone\", \"cartilage\": \"Bone\",\n",
        "    \"bone marrow\": \"Bone marrow\",\n",
        "    \"hair\": \"Hair\",\n",
        "    \"nail\": \"Nail\",\n",
        "\n",
        "    # Nervous / Sensory\n",
        "    \"eye\": \"Eye\", \"eyes\": \"Eye\", \"retina\": \"Eye\",\n",
        "    \"olfactory epithelium\": \"Olfactory epithelium\", \"olfactory rosette\": \"Olfactory epithelium\",\n",
        "    \"spinal cord\": \"Spinal cord\",\n",
        "\n",
        "    # General / Composite / PBPK Buckets\n",
        "    \"carcass\": \"Carcass\", \"carcass remainder\": \"Carcass\",\n",
        "    \"whole body\": \"Whole body\", \"whole organism\": \"Whole body\", \"whole pup\": \"Whole body\",\n",
        "    \"rest of body\": \"Rest of body\", \"rest of body tissues\": \"Rest of body\",\n",
        "    \"soft tissue\": \"Soft tissue\", \"viscera\": \"Viscera\", \"visceral mass\": \"Viscera\",\n",
        "    \"head\": \"Head\",\n",
        "    \"extremities\": \"Extremities\",\n",
        "    \"filtrate\": \"Filtrate\",\n",
        "    \"storage\": \"Storage\",\n",
        "    \"cell\": \"Cells\",\n",
        "    \"tissue\": \"Tissue\",\n",
        "\n",
        "    # Immune / Lymph\n",
        "    \"thymus\": \"Thymus\",\n",
        "    \"lymph node\": \"Lymph node\", \"lymph nodes\": \"Lymph node\", \"thoracic lymph nodes\": \"Lymph node\", \"mesenteric lymph nodes\": \"Lymph node\",\n",
        "    \"immune\": \"Immune system\",\n",
        "\n",
        "    # Plants (from your list)\n",
        "    \"root\": \"Root\",\n",
        "    \"shoot\": \"Shoot\",\n",
        "\n",
        "    # Invertebrate specifics\n",
        "    \"digestive gland\": \"Hepatopancreas\", # Often functionally equivalent to liver/pancreas in mollusks\n",
        "    \"mantle\": \"Mantle\",\n",
        "    \"adductor muscle\": \"Muscle\",\n",
        "    \"shell\": \"Shell\",\n",
        "\n",
        "    # Misc\n",
        "    \"gland\": \"Gland\",\n",
        "    \"tumour\": \"Tumor\",\n",
        "    \"mandible\": \"Bone\",\n",
        "    \"palatal shelves\": \"Bone\",\n",
        "}\n",
        "\n",
        "# ======================================\n",
        "# Color palettes\n",
        "# ======================================\n",
        "\n",
        "# 1) Individual Top-20 bars (PFAS / species / organs)\n",
        "# 8 base colors from the rainbow palette\n",
        "BASE_RAINBOW_HEX = [\n",
        "    \"#f57c6e\",  # red-orange\n",
        "    \"#f2b56e\",  # orange\n",
        "    \"#fbe79e\",  # yellow\n",
        "    \"#84c3b7\",  # green-teal\n",
        "    \"#88d7da\",  # light cyan\n",
        "    \"#71b8ed\",  # blue\n",
        "    \"#b8aeea\",  # lavender\n",
        "    \"#f2a8da\",  # pink\n",
        "]\n",
        "\n",
        "# Build a smooth colormap from these 8 colors\n",
        "RAINBOW_CMAP = LinearSegmentedColormap.from_list(\n",
        "    \"soft_rainbow\", BASE_RAINBOW_HEX, N=256\n",
        ")\n",
        "\n",
        "# Sample 20 evenly spaced colors from the colormap\n",
        "BAR_PALETTE_INDIV = [\n",
        "    mcolors.to_hex(RAINBOW_CMAP(x))\n",
        "    for x in np.linspace(0, 1, 20)\n",
        "]\n",
        "\n",
        "# Use this as your default palette for individual Top-20 plots\n",
        "BAR_PALETTE = BAR_PALETTE_INDIV\n",
        "\n",
        "# 2) Species groups (7 categories including \"Other\")\n",
        "BAR_PALETTE_SPECIES_GROUP = [\n",
        "    \"#4E659B\",  # 078,101,155\n",
        "    \"#8A8CBF\",  # 138,140,191\n",
        "    \"#B8A8CF\",  # 184,168,207\n",
        "    \"#E7BCC6\",  # 231,188,198\n",
        "    \"#FDCF9E\",  # 253,207,158\n",
        "    \"#EFA484\",  # 239,164,132\n",
        "    \"#B6766C\",  # 182,118,108\n",
        "]\n",
        "\n",
        "# 3) Organ groups (10 categories including \"Other\")\n",
        "BAR_PALETTE_ORGAN_GROUP = [\n",
        "    \"#E76254\",  # 231, 98, 84\n",
        "    \"#EF8A47\",  # 239,138, 71\n",
        "    \"#F7AA58\",  # 247,170, 88\n",
        "    \"#FFD06F\",  # 255,208,111\n",
        "    \"#FFE6B7\",  # 255,230,183\n",
        "    \"#AAE6E0\",  # 170,230,224\n",
        "    \"#72BCD5\",  # 114,188,213\n",
        "    \"#528FAD\",  #  82,143,173\n",
        "    \"#376795\",  #  55,103,149\n",
        "    \"#1E466E\",  #  30, 70,110\n",
        "]\n",
        "\n",
        "# Default palette used by barplot_topn for individual plots\n",
        "BAR_PALETTE = BAR_PALETTE_INDIV\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Parsing / cleaning utils\n",
        "# =========================\n",
        "SEMICOLON_SPLIT_RE = re.compile(r\"\\s*;\\s*\")\n",
        "ABBR_RE = re.compile(r\"^(.*?)\\s*\\(([^)]+)\\)\\s*$\")\n",
        "\n",
        "def _split_semicolon(cell: str):\n",
        "    if not isinstance(cell, str) or not cell.strip():\n",
        "        return []\n",
        "    return [p.strip() for p in SEMICOLON_SPLIT_RE.split(cell.strip()) if p.strip()]\n",
        "\n",
        "def clean_chemical(token: str) -> str:\n",
        "    \"\"\"\n",
        "    If token contains 'Full name (ABBR)', return ABBR (uppercased).\n",
        "    Else if it's likely an abbreviation, uppercase; else Title Case full name.\n",
        "    \"\"\"\n",
        "    t = (token or \"\").strip()\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    m = ABBR_RE.match(t)\n",
        "    if m:\n",
        "        abbr = re.sub(r\"[^A-Za-z0-9\\-]\", \"\", m.group(2).strip())\n",
        "        return abbr.upper() if abbr else m.group(1).strip().title()\n",
        "    if \" \" not in t and len(t) <= 12:\n",
        "        cleaned = re.sub(r\"[^A-Za-z0-9\\-]\", \"\", t)\n",
        "        if cleaned:\n",
        "            return cleaned.upper()\n",
        "    return re.sub(r\"\\s+\", \" \", t.lower()).strip().title()\n",
        "\n",
        "def clean_category(token: str, mapping: dict | None = None) -> str:\n",
        "    t = (token or \"\").strip()\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    low = t.lower()\n",
        "    if mapping and low in mapping:\n",
        "        return mapping[low]\n",
        "    return re.sub(r\"\\s+\", \" \", t).strip().title()\n",
        "\n",
        "def explode_multi(df_in: pd.DataFrame, col: str, cleaner, mapping=None) -> pd.DataFrame:\n",
        "    tmp = df_in[[PMID_COL, col]].copy()\n",
        "    tmp[col] = tmp[col].apply(_split_semicolon)\n",
        "    tmp = tmp.explode(col)\n",
        "    tmp[col] = tmp[col].fillna(\"\").astype(str)\n",
        "    tmp[col] = tmp[col].apply(lambda x: cleaner(x) if mapping is None else cleaner(x, mapping))\n",
        "    tmp = tmp[tmp[col].str.len() > 0]\n",
        "    return tmp\n",
        "\n",
        "# =========================\n",
        "# Plot helpers\n",
        "# =========================\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# CONFIGURATION: Set DPI to 300\n",
        "# ==========================================\n",
        "SAVE_DPI = 300\n",
        "SAVE_KW = dict(format=\"tiff\", dpi=SAVE_DPI, pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
        "\n",
        "# Update global matplotlib settings for consistency\n",
        "plt.rcParams.update({\n",
        "    \"savefig.dpi\": SAVE_DPI,\n",
        "    \"figure.dpi\": 110,  # Screen resolution\n",
        "})\n",
        "\n",
        "# ==========================================\n",
        "# FUNCTION DEFINITION (Saves TIFF & PNG)\n",
        "# ==========================================\n",
        "def barplot_topn(counts: pd.Series,\n",
        "                 title: str,\n",
        "                 xlabel: str,\n",
        "                 ylabel: str,\n",
        "                 save_path: str,\n",
        "                 palette=None):\n",
        "    \"\"\"\n",
        "    Vertical bar plot for top N categories.\n",
        "    Saves in BOTH .tiff and .png formats automatically.\n",
        "    \"\"\"\n",
        "    # Print counts for verification\n",
        "    print(f\"\\n[Record counts] {title} (all categories):\")\n",
        "    print(counts.to_string())\n",
        "\n",
        "    # Prepare data\n",
        "    top_counts = counts.head(TOPN)\n",
        "    n = len(top_counts)\n",
        "    if palette is None:\n",
        "        palette = BAR_PALETTE\n",
        "    palette_use = palette[:n]\n",
        "\n",
        "    # Create figure\n",
        "    fig_w = max(8, min(28, 0.50 * n + 2))\n",
        "    plt.figure(figsize=(fig_w, 7))\n",
        "\n",
        "    # Plot (Vertical)\n",
        "    ax = sns.barplot(\n",
        "        x=top_counts.index,\n",
        "        y=top_counts.values,\n",
        "        hue=top_counts.index,\n",
        "        palette=palette_use\n",
        "    )\n",
        "\n",
        "    # Add labels on top of bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%d', padding=3, fontsize=9)\n",
        "\n",
        "    # Adjust axes\n",
        "    ymax = top_counts.values.max()\n",
        "    ax.set_ylim(0, ymax * 1.15)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    # Add Titles/Labels\n",
        "    ax.set_xlabel(xlabel, fontsize=16, fontweight='bold')\n",
        "    ax.set_ylabel(ylabel, fontsize=16, fontweight='bold')\n",
        "    ax.set_title(title, fontsize=20, pad=12, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # --- SAVE LOGIC ---\n",
        "    # 1. Save the TIFF (using the path you provided in the call)\n",
        "    plt.savefig(save_path, **SAVE_KW)\n",
        "    print(f\"Saved TIFF: {save_path}\")\n",
        "\n",
        "    # 2. Automatically save the PNG (derived from the same filename)\n",
        "    base_name, _ = os.path.splitext(save_path)\n",
        "    png_path = f\"{base_name}.png\"\n",
        "    plt.savefig(png_path, format=\"png\", dpi=SAVE_DPI)\n",
        "    print(f\"Saved PNG:  {png_path}\")\n",
        "    # ------------------\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def sort_with_other_last(counts: pd.Series, other_label=\"Other\") -> pd.Series:\n",
        "    \"\"\"Sort counts descending, but force 'Other' to be last if present.\"\"\"\n",
        "    if other_label in counts.index:\n",
        "        main = counts.drop(other_label).sort_values(ascending=False)\n",
        "        return pd.concat([main, counts[[other_label]]])\n",
        "    else:\n",
        "        return counts.sort_values(ascending=False)\n",
        "\n",
        "def plot_heatmap_varscale(\n",
        "    M,\n",
        "    title,\n",
        "    xlabel, ylabel,\n",
        "    save_path,\n",
        "    mode=\"log\",\n",
        "    cmap=\"coolwarm\",\n",
        "    annotate=True,\n",
        "    dpi_save_kwargs=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize skewed count matrices with alternative scalings while annotating true counts.\n",
        "    \"\"\"\n",
        "    if dpi_save_kwargs is None:\n",
        "        dpi_save_kwargs = dict(format=\"tiff\", dpi=600, pil_kwargs={\"compression\": \"tiff_lzw\"})\n",
        "\n",
        "    counts = M.astype(int).copy()\n",
        "    data   = counts.copy().astype(float)\n",
        "    cbar_label = \"Count\"\n",
        "\n",
        "    if mode == \"abs\":\n",
        "        vmin, vmax = 0, data.values.max()\n",
        "\n",
        "    elif mode == \"log\":\n",
        "        data = np.log10(data + 1.0)\n",
        "        vmin, vmax = 0, data.values.max()\n",
        "        cbar_label = \"log10(count + 1)\"\n",
        "\n",
        "    elif mode == \"sqrt\":\n",
        "        data = np.sqrt(data)\n",
        "        vmin, vmax = 0, data.values.max()\n",
        "        cbar_label = \"sqrt(count)\"\n",
        "\n",
        "    elif mode == \"clip95\":\n",
        "        vmin, vmax = 0, np.percentile(data.values, 95)\n",
        "        cbar_label = \"Count (vmax = 95th pct)\"\n",
        "\n",
        "    elif mode == \"quantile\":\n",
        "        # 10 quantile bins; adjust if you want more/less bands\n",
        "        qs = np.linspace(0, 100, 11)\n",
        "        bounds = np.percentile(data.values.ravel(), qs)\n",
        "        # ensure unique, slight jitter if needed\n",
        "        bounds = np.unique(bounds)\n",
        "        if len(bounds) < 3:  # degenerate case\n",
        "            bounds = np.array([0, 1, data.values.max()])\n",
        "        norm = BoundaryNorm(bounds, ncolors=plt.get_cmap(cmap).N, clip=True)\n",
        "        vmin = vmax = None   # handled by norm\n",
        "    elif mode == \"row_pct\":\n",
        "        row_sums = data.sum(axis=1).replace(0, np.nan)\n",
        "        pct = data.div(row_sums, axis=0) * 100.0\n",
        "        data = pct.fillna(0.0)\n",
        "        vmin, vmax = 0, 100\n",
        "        cbar_label = \"Row %\"\n",
        "    else:\n",
        "        raise ValueError(\"mode must be one of: abs, log, sqrt, clip95, quantile, row_pct\")\n",
        "\n",
        "    # figure size\n",
        "    h, w = data.shape\n",
        "    fig_w = max(10, min(28, 0.50 * w + 6))\n",
        "    fig_h = max(8,  min(28, 0.50 * h + 4))\n",
        "\n",
        "    plt.figure(figsize=(fig_w, fig_h))\n",
        "    if mode == \"quantile\":\n",
        "        ax = sns.heatmap(\n",
        "            data, annot=annotate, fmt=\"d\" if mode!=\"row_pct\" else \".0f\",\n",
        "            cmap=cmap, norm=norm, linewidths=0.2, linecolor=\"white\",\n",
        "            cbar_kws={\"label\": cbar_label}\n",
        "        )\n",
        "    else:\n",
        "        ax = sns.heatmap(\n",
        "            data, annot=annotate, fmt=\"d\" if mode!=\"row_pct\" else \".0f\",\n",
        "            cmap=cmap, vmin=vmin, vmax=vmax, linewidths=0.2, linecolor=\"white\",\n",
        "            cbar_kws={\"label\": cbar_label}\n",
        "        )\n",
        "\n",
        "    # If using row_pct, annotate with counts but color by %\n",
        "    if mode == \"row_pct\" and annotate:\n",
        "        for (i, j), _ in np.ndenumerate(data.values):\n",
        "            ax.text(\n",
        "                j + 0.5, i + 0.5,\n",
        "                f\"{counts.iat[i, j]}\",     # show counts\n",
        "                ha=\"center\", va=\"center\", color=\"black\", fontsize=8\n",
        "            )\n",
        "\n",
        "    ax.set_title(title, pad=12)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, **dpi_save_kwargs)\n",
        "    plt.show()\n",
        "\n",
        "# =========================\n",
        "# Normalize & explode\n",
        "# =========================\n",
        "chem_long = explode_multi(df, CHEM_COL, cleaner=clean_chemical)\n",
        "species_long = explode_multi(df, SPECIES_COL, cleaner=clean_category, mapping=SPECIES_NORMALIZE)\n",
        "organs_long  = explode_multi(df, ORGANS_COL,  cleaner=clean_category, mapping=ORGANS_NORMALIZE)"
      ],
      "metadata": {
        "id": "1NhEDefLyjo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 3"
      ],
      "metadata": {
        "id": "9kn4UHGoypVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import pandas as pd\n",
        "import numpy as np # Added for color calculation\n",
        "\n",
        "# CHANGE THIS to match your actual year column\n",
        "YEAR_COL = \"publication_year\"\n",
        "\n",
        "# Clean year data\n",
        "years = pd.to_numeric(df[YEAR_COL], errors=\"coerce\").dropna().astype(int)\n",
        "year_distribution = years.value_counts().sort_index()\n",
        "\n",
        "# Color Visibility\n",
        "num_years = len(year_distribution)\n",
        "color_indices = np.linspace(0.3, 1.0, num_years)\n",
        "gradient_colors = cm.Blues(color_indices)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "bars = plt.bar(\n",
        "    year_distribution.index.astype(str),\n",
        "    year_distribution.values,\n",
        "    color=gradient_colors\n",
        ")\n",
        "\n",
        "plt.title(\"Distribution of PFAS Publications by Year\", fontsize=20, weight=\"bold\")\n",
        "plt.xlabel(\"Publication Year\", fontsize=16, weight=\"bold\")\n",
        "plt.ylabel(\"Number of Articles\", fontsize=16, weight=\"bold\")\n",
        "\n",
        "plt.xticks(rotation=75, fontsize=14)\n",
        "plt.ylim(0, year_distribution.max() + 10)\n",
        "\n",
        "# Remove Top and Right Spines\n",
        "ax = plt.gca() # Get current axes\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "\n",
        "# Add counts above each bar\n",
        "for bar, value in zip(bars, year_distribution.values):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        bar.get_height() + 0.5,\n",
        "        str(value),\n",
        "        ha='center',\n",
        "        va='bottom',\n",
        "        fontsize=12\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save TIFF (publication quality)\n",
        "plt.savefig(\"PFAS_PublicationYear_Distribution.tiff\", dpi=600, format=\"tiff\")\n",
        "plt.savefig(\"PFAS_PublicationYear_Distribution.png\", dpi=600, format=\"png\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "year_distribution"
      ],
      "metadata": {
        "id": "CweSz-31yuJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 4"
      ],
      "metadata": {
        "id": "zqkeRNAKyq52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load species groups from species_list.txt\n",
        "with open(\"species_list.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    SPECIES_GROUPS = json.load(f)\n",
        "\n",
        "# Build species → group map (case-insensitive)\n",
        "SPECIES_TO_GROUP = {}\n",
        "for group, names in SPECIES_GROUPS.items():\n",
        "    for name in names:\n",
        "        SPECIES_TO_GROUP[name.strip().lower()] = group\n",
        "\n",
        "def map_species_group(name: str) -> str:\n",
        "    low = str(name).strip().lower()\n",
        "    return SPECIES_TO_GROUP.get(low, \"Other\")\n",
        "\n",
        "# Assign groups\n",
        "species_long[\"Species_group\"] = species_long[SPECIES_COL].apply(map_species_group)\n",
        "\n",
        "# Count records per group (deduplicate by PMID × group)\n",
        "species_group_counts = (\n",
        "    species_long\n",
        "    .drop_duplicates([PMID_COL, \"Species_group\"])\n",
        "    [\"Species_group\"]\n",
        "    .value_counts()\n",
        ")\n",
        "\n",
        "# Sort by count, but keep \"Other\" last\n",
        "species_group_counts_sorted = sort_with_other_last(species_group_counts, other_label=\"Other\")\n",
        "\n",
        "# Plot grouped species with 7-color palette\n",
        "barplot_topn(\n",
        "    species_group_counts_sorted,\n",
        "    title=\"Species Category Count by Group\",\n",
        "    xlabel=\"Species Group\",\n",
        "    ylabel=\"Number of Records\",\n",
        "    save_path=\"Species_grouped.png\",\n",
        "    palette=BAR_PALETTE_SPECIES_GROUP,\n",
        ")"
      ],
      "metadata": {
        "id": "YDGGXAQdyvkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load organ/tissue groups from organs_tissues_list.txt\n",
        "with open(\"organs_tissues_list.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ORGANS_GROUPS = json.load(f)\n",
        "\n",
        "# Build organ → group map (case-insensitive)\n",
        "ORG_TO_GROUP = {}\n",
        "for group, names in ORGANS_GROUPS.items():\n",
        "    for name in names:\n",
        "        ORG_TO_GROUP[name.strip().lower()] = group\n",
        "\n",
        "def map_organ_group(name: str) -> str:\n",
        "    low = str(name).strip().lower()\n",
        "    return ORG_TO_GROUP.get(low, \"Other\")\n",
        "\n",
        "# Assign groups\n",
        "organs_long[\"Organ_group\"] = organs_long[ORGANS_COL].apply(map_organ_group)\n",
        "\n",
        "# Count records per organ group (deduplicate by PMID × group)\n",
        "organ_group_counts = (\n",
        "    organs_long\n",
        "    .drop_duplicates([PMID_COL, \"Organ_group\"])\n",
        "    [\"Organ_group\"]\n",
        "    .value_counts()\n",
        ")\n",
        "\n",
        "# Sort with \"Other\" last\n",
        "organ_group_counts_sorted = sort_with_other_last(organ_group_counts, other_label=\"Other\")\n",
        "\n",
        "# Plot grouped organs with 10-color palette\n",
        "barplot_topn(\n",
        "    organ_group_counts_sorted,\n",
        "    title=\"Organ Category Count by Group\",\n",
        "    xlabel=\"Organ Group\",\n",
        "    ylabel=\"Number of Records\",\n",
        "    save_path=\"Organs_grouped.png\",\n",
        "    palette=BAR_PALETTE_ORGAN_GROUP,\n",
        ")"
      ],
      "metadata": {
        "id": "NNjpduAnyw0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 5"
      ],
      "metadata": {
        "id": "q7t7h-l5yruK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Individual Bar Plots (Top-20) ===\n",
        "\n",
        "# 1. PFAS\n",
        "chem_counts = (\n",
        "    chem_long.drop_duplicates([PMID_COL, CHEM_COL])[CHEM_COL]\n",
        "    .value_counts()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "barplot_topn(\n",
        "    chem_counts,\n",
        "    title=\"PFAS Category Count\",\n",
        "    xlabel=\"PFAS\",\n",
        "    ylabel=\"Number of Records\",\n",
        "    save_path=\"PFAS_category_count_top20_vertical.tiff\",\n",
        "    # The function will automatically create \"PFAS_category_count_top20_vertical.png\"\n",
        ")\n",
        "\n",
        "# 2. Species\n",
        "species_counts = (\n",
        "    species_long.drop_duplicates([PMID_COL, SPECIES_COL])[SPECIES_COL]\n",
        "    .value_counts()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "barplot_topn(\n",
        "    species_counts,\n",
        "    title=\"Species Category Count\",\n",
        "    xlabel=\"Species\",\n",
        "    ylabel=\"Number of Records\",\n",
        "    save_path=\"Species_individual_top20_vertical.tiff\",\n",
        ")\n",
        "\n",
        "# 3. Organs\n",
        "org_counts = (\n",
        "    organs_long.drop_duplicates([PMID_COL, ORGANS_COL])[ORGANS_COL]\n",
        "    .value_counts()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "barplot_topn(\n",
        "    org_counts,\n",
        "    title=\"Organs Category Count\",\n",
        "    xlabel=\"Organ\",\n",
        "    ylabel=\"Number of Records\",\n",
        "    save_path=\"Organs_individual_top20_vertical.tiff\",\n",
        ")"
      ],
      "metadata": {
        "id": "Enrvo79XyylA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 6"
      ],
      "metadata": {
        "id": "ySyedVH4yseQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PFAS x Species"
      ],
      "metadata": {
        "id": "JEniitsOyzy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ========================\n",
        "# 1. Build PFAS × Species_group table\n",
        "# ========================\n",
        "pairs_pfas_speciesgrp = (\n",
        "    chem_long[[PMID_COL, CHEM_COL]]\n",
        "    .merge(\n",
        "        species_long[[PMID_COL, \"Species_group\"]],\n",
        "        on=PMID_COL,\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .drop_duplicates(subset=[PMID_COL, CHEM_COL, \"Species_group\"])\n",
        "    .rename(columns={CHEM_COL: \"PFAS\"})\n",
        ")\n",
        "\n",
        "pfas_species_counts = (\n",
        "    pairs_pfas_speciesgrp\n",
        "    .groupby([\"PFAS\", \"Species_group\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "# Choose top-N PFAS by total records\n",
        "TOPN_PFAS_FOR_STACK = 16\n",
        "top_pfas = (\n",
        "    pfas_species_counts\n",
        "    .groupby(\"PFAS\")[\"count\"].sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(TOPN_PFAS_FOR_STACK)\n",
        "    .index\n",
        ")\n",
        "\n",
        "pfas_species_top = pfas_species_counts[pfas_species_counts[\"PFAS\"].isin(top_pfas)]\n",
        "\n",
        "pfas_species_pivot = (\n",
        "    pfas_species_top\n",
        "    .pivot(index=\"PFAS\", columns=\"Species_group\", values=\"count\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Order PFAS by total count\n",
        "pfas_species_pivot = pfas_species_pivot.loc[\n",
        "    pfas_species_pivot.sum(axis=1).sort_values(ascending=False).index\n",
        "]\n",
        "\n",
        "# ========================\n",
        "# 2. Force species order to match Figure 1\n",
        "# ========================\n",
        "species_order = [\n",
        "    \"Humans\",\n",
        "    \"Experimental Animals\",\n",
        "    \"Aquatic Species\",\n",
        "    \"Livestock\",\n",
        "    \"Pets\",\n",
        "    \"Wildlife\",\n",
        "    \"Other\",\n",
        "]\n",
        "\n",
        "# Only keep those species groups that actually appear\n",
        "cols_in_order = [s for s in species_order if s in pfas_species_pivot.columns]\n",
        "\n",
        "# Reorder columns in pivot table\n",
        "pfas_species_pivot = pfas_species_pivot[cols_in_order]\n",
        "\n",
        "# ========================\n",
        "# 3. Colors from BAR_PALETTE_SPECIES_GROUP\n",
        "# ========================\n",
        "# (must be in the same order as `species_order`)\n",
        "species_color_map = {\n",
        "    group: color\n",
        "    for group, color in zip(species_order, BAR_PALETTE_SPECIES_GROUP)\n",
        "}\n",
        "\n",
        "species_colors_for_cols = [species_color_map[c] for c in cols_in_order]\n",
        "\n",
        "# ========================\n",
        "# 4. Plot stacked bar\n",
        "# ========================\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = pfas_species_pivot.plot(\n",
        "    kind=\"bar\",\n",
        "    stacked=True,\n",
        "    color=species_colors_for_cols,\n",
        "    figsize=(12, 6)\n",
        ")\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.ylabel(\"Number of Records\", fontsize=12, fontweight=\"bold\")\n",
        "plt.xlabel(\"PFAS\", fontsize=12, fontweight=\"bold\")\n",
        "plt.title(\"Species Group Distribution for Top PFAS\", fontsize=14, fontweight=\"bold\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(\n",
        "    title=\"Species Group\",\n",
        "    bbox_to_anchor=(1.02, 1),\n",
        "    loc=\"upper left\"\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"PFAS_SpeciesGroup_stacked.png\", dpi=300, format=\"png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JgGiCqzuy0R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PFAS x Organs"
      ],
      "metadata": {
        "id": "MklmFImBy1fW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ========================\n",
        "# 1. Build PFAS × Organs_group table\n",
        "# ========================\n",
        "pairs_pfas_organsgrp = (\n",
        "    chem_long[[PMID_COL, CHEM_COL]]\n",
        "    .merge(\n",
        "        organs_long[[PMID_COL, \"Organ_group\"]],\n",
        "        on=PMID_COL,\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .drop_duplicates(subset=[PMID_COL, CHEM_COL, \"Organ_group\"])\n",
        "    .rename(columns={CHEM_COL: \"PFAS\"})\n",
        ")\n",
        "\n",
        "pfas_organs_counts = (\n",
        "    pairs_pfas_organsgrp\n",
        "    .groupby([\"PFAS\", \"Organ_group\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "# Choose top-N PFAS by total records\n",
        "TOPN_PFAS_FOR_STACK = 16\n",
        "top_pfas = (\n",
        "    pfas_organs_counts\n",
        "    .groupby(\"PFAS\")[\"count\"].sum()\n",
        "    .sort_values(ascending=False)\n",
        "    .head(TOPN_PFAS_FOR_STACK)\n",
        "    .index\n",
        ")\n",
        "\n",
        "pfas_organs_top = pfas_organs_counts[pfas_organs_counts[\"PFAS\"].isin(top_pfas)]\n",
        "\n",
        "pfas_organs_pivot = (\n",
        "    pfas_organs_top\n",
        "    .pivot(index=\"PFAS\", columns=\"Organ_group\", values=\"count\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Order PFAS by total count\n",
        "pfas_organs_pivot = pfas_organs_pivot.loc[\n",
        "    pfas_organs_pivot.sum(axis=1).sort_values(ascending=False).index\n",
        "]\n",
        "\n",
        "# ========================\n",
        "# 2. Force organs order\n",
        "# ========================\n",
        "organs_order = [\n",
        "    \"Major Organs\",\n",
        "    \"Blood and Circulation\",\n",
        "    \"Other Tissues\",\n",
        "    \"Reproductive Organs\",\n",
        "    \"Immune System\",\n",
        "    \"Digestive System\",\n",
        "    \"Respiratory System\",\n",
        "    \"Other\",\n",
        "]\n",
        "\n",
        "# Only keep those organs groups that actually appear\n",
        "cols_in_order = [s for s in organs_order if s in pfas_organs_pivot.columns]\n",
        "\n",
        "# Reorder columns in pivot table\n",
        "pfas_organs_pivot = pfas_organs_pivot[cols_in_order]\n",
        "\n",
        "# ========================\n",
        "# 3. Colors from BAR_PALETTE_ORGAN_GROUP\n",
        "# ========================\n",
        "# (must be in the same order as `organs_order`)\n",
        "organs_color_map = {\n",
        "    group: color\n",
        "    for group, color in zip(organs_order, BAR_PALETTE_ORGAN_GROUP)\n",
        "}\n",
        "\n",
        "organs_colors_for_cols = [organs_color_map[c] for c in cols_in_order]\n",
        "\n",
        "# ========================\n",
        "# 4. Plot stacked bar\n",
        "# ========================\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = pfas_organs_pivot.plot(\n",
        "    kind=\"bar\",\n",
        "    stacked=True,\n",
        "    color=organs_colors_for_cols,\n",
        "    figsize=(12, 6)\n",
        ")\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "plt.ylabel(\"Number of Records\", fontsize=12, fontweight=\"bold\")\n",
        "plt.xlabel(\"PFAS\", fontsize=12, fontweight=\"bold\")\n",
        "plt.title(\"Organs Group Distribution for Top PFAS\", fontsize=14, fontweight=\"bold\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.legend(\n",
        "    title=\"Organs Group\",\n",
        "    bbox_to_anchor=(1.02, 1),\n",
        "    loc=\"upper left\"\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"PFAS_OrgansGroup_stacked.tiff\", **SAVE_KW)\n",
        "plt.savefig(\"PFAS_OrgansGroup_stacked.png\", dpi=300, format=\"png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cE3Bf2pry20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Species x Organs"
      ],
      "metadata": {
        "id": "6KXDkM-iy37x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# ========================\n",
        "# 1. Build Species_group × Organ_group table\n",
        "# ========================\n",
        "\n",
        "pairs_speciesgrp_organgrp = (\n",
        "    species_long[[PMID_COL, \"Species_group\"]]\n",
        "    .merge(\n",
        "        organs_long[[PMID_COL, \"Organ_group\"]],\n",
        "        on=PMID_COL,\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    .drop_duplicates(subset=[PMID_COL, \"Species_group\", \"Organ_group\"])\n",
        ")\n",
        "\n",
        "species_org_counts = (\n",
        "    pairs_speciesgrp_organgrp\n",
        "    .groupby([\"Species_group\", \"Organ_group\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "species_org_matrix = (\n",
        "    species_org_counts\n",
        "    .pivot(index=\"Species_group\", columns=\"Organ_group\", values=\"count\")\n",
        "    .fillna(0)\n",
        ")\n",
        "\n",
        "# Order species groups (X-axis categories)\n",
        "species_order = [\n",
        "    \"Humans\",\n",
        "    \"Experimental Animals\",\n",
        "    \"Aquatic Species\",\n",
        "    \"Livestock\",\n",
        "    \"Pets\",\n",
        "    \"Wildlife\",\n",
        "    \"Other\",\n",
        "]\n",
        "rows_in_order = [s for s in species_order if s in species_org_matrix.index]\n",
        "species_org_matrix = species_org_matrix.loc[rows_in_order]\n",
        "\n",
        "# ========================\n",
        "# 2. Force organ group order (Legend/Stack order)\n",
        "# ========================\n",
        "organ_order = [\n",
        "    \"Major Organs\",\n",
        "    \"Blood and Circulation\",\n",
        "    \"Other Tissues\",\n",
        "    \"Common Test Matrices\",\n",
        "    \"Reproductive Organs\",\n",
        "    \"Immune System\",\n",
        "    \"Digestive System\",\n",
        "    \"Respiratory System\",\n",
        "    \"Other\",\n",
        "]\n",
        "cols_in_order_org = [o for o in organ_order if o in species_org_matrix.columns]\n",
        "species_org_matrix = species_org_matrix[cols_in_order_org]\n",
        "\n",
        "# ========================\n",
        "# 3. Map organ colors\n",
        "# ========================\n",
        "organ_color_map = {\n",
        "    group: color\n",
        "    for group, color in zip(organ_order, BAR_PALETTE_ORGAN_GROUP)\n",
        "}\n",
        "organ_colors_for_cols = [organ_color_map[c] for c in cols_in_order_org]\n",
        "\n",
        "# ========================\n",
        "# 4. Stacked Vertical Barplot\n",
        "# ========================\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "ax = species_org_matrix.plot(\n",
        "    kind=\"bar\",\n",
        "    stacked=True,\n",
        "    color=organ_colors_for_cols,\n",
        "    figsize=(10, 7)\n",
        ")\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "# --- Updated Axis Labels ---\n",
        "plt.xlabel(\"Species Group\", fontsize=12, fontweight=\"bold\")\n",
        "plt.ylabel(\"Number of Records\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "plt.title(\"Distribution of Organ Groups within Species Groups\",\n",
        "          fontsize=14, fontweight=\"bold\", pad=20)\n",
        "\n",
        "plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save in both formats at 300 DPI\n",
        "plt.savefig(\"SpeciesGroup_to_OrganGroup_stacked_ver.png\", dpi=300, format=\"png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lKct134Ay5RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Figure 7"
      ],
      "metadata": {
        "id": "mYpmVqdmytPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "# -----------------------------\n",
        "# Font sizes\n",
        "# -----------------------------\n",
        "HM_TITLE_FS   = 22\n",
        "HM_XLABEL_FS  = 22\n",
        "HM_YLABEL_FS  = 22\n",
        "HM_TICK_FS    = 16\n",
        "HM_ANNOT_FS   = 14\n",
        "HM_CBAR_LBLFS = 18\n",
        "HM_CBAR_TICK  = 14\n",
        "\n",
        "# ======================================\n",
        "# 1) Plot Function (generic y-label)\n",
        "# ======================================\n",
        "def plot_top20_heatmap(ct: pd.DataFrame,\n",
        "                       x_label: str,\n",
        "                       y_label: str,\n",
        "                       filename: str,\n",
        "                       title_prefix: str,\n",
        "                       topn: int = None):\n",
        "    \"\"\"\n",
        "    Plot a Top-N x Top-N heatmap using log color scaling.\n",
        "    - Colors use log scale on (count + 1)\n",
        "    - Annotations show TRUE counts (no +1)\n",
        "    \"\"\"\n",
        "    if topn is None:\n",
        "        topn = TOPN\n",
        "\n",
        "    # Select Top-N based on row/col sums\n",
        "    rows = ct.sum(axis=1).sort_values(ascending=False).head(topn).index\n",
        "    cols = ct.sum(axis=0).sort_values(ascending=False).head(topn).index\n",
        "    sub = ct.loc[rows, cols].copy()\n",
        "\n",
        "    # If matrix is empty, fail gracefully\n",
        "    if sub.empty:\n",
        "        print(f\"[Skip] {title_prefix}: empty matrix after Top-{topn} filtering.\")\n",
        "        return\n",
        "\n",
        "    # Dynamic figure size\n",
        "    h, w = sub.shape\n",
        "    fig_w = max(10, min(28, 0.50 * w + 6))\n",
        "    fig_h = max(8,  min(28, 0.50 * h + 4))\n",
        "\n",
        "    # Color setup\n",
        "    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
        "\n",
        "    # LogNorm requires positive values\n",
        "    vmax = int(sub.values.max()) + 1\n",
        "    norm = LogNorm(vmin=1, vmax=max(2, vmax))\n",
        "\n",
        "    plt.figure(figsize=(fig_w, fig_h))\n",
        "    ax = sns.heatmap(\n",
        "        sub + 1,\n",
        "        annot=sub, fmt=\"d\",\n",
        "        cmap=cmap,\n",
        "        norm=norm,\n",
        "        linewidths=0.25,\n",
        "        linecolor=\"white\",\n",
        "        annot_kws={\"fontsize\": HM_ANNOT_FS},\n",
        "        cbar_kws={\"label\": \"Count\"}\n",
        "    )\n",
        "\n",
        "    # Title and labels\n",
        "    ax.set_title(title_prefix, fontsize=HM_TITLE_FS, fontweight=\"bold\", pad=16)\n",
        "    ax.set_xlabel(x_label, fontsize=HM_XLABEL_FS, fontweight=\"bold\", labelpad=20)\n",
        "    ax.set_ylabel(y_label, fontsize=HM_YLABEL_FS, fontweight=\"bold\")\n",
        "\n",
        "    # Tick labels\n",
        "    ax.tick_params(axis=\"x\", labelsize=HM_TICK_FS)\n",
        "    ax.tick_params(axis=\"y\", labelsize=HM_TICK_FS)\n",
        "\n",
        "    # Colorbar styling\n",
        "    cbar = ax.collections[0].colorbar\n",
        "    cbar.ax.set_ylabel(\"Count\", fontsize=HM_CBAR_LBLFS, fontweight=\"bold\")\n",
        "    cbar.ax.tick_params(labelsize=HM_CBAR_TICK)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename, **SAVE_KW)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2) Build matrices and plot\n",
        "# ============================================================\n",
        "\n",
        "# -----------------------------\n",
        "# PFAS × Organs\n",
        "# -----------------------------\n",
        "pairs_chem_org = (\n",
        "    chem_long[[PMID_COL, CHEM_COL]]\n",
        "    .merge(organs_long[[PMID_COL, ORGANS_COL]], on=PMID_COL, how=\"inner\")\n",
        "    .drop_duplicates(subset=[PMID_COL, CHEM_COL, ORGANS_COL])\n",
        "    .rename(columns={CHEM_COL: \"PFAS\", ORGANS_COL: \"Organ\"})\n",
        ")\n",
        "\n",
        "ct_chem_org = (\n",
        "    pairs_chem_org\n",
        "    .groupby([\"PFAS\", \"Organ\"], as_index=False)\n",
        "    .size()\n",
        "    .pivot(index=\"PFAS\", columns=\"Organ\", values=\"size\")\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "plot_top20_heatmap(\n",
        "    ct_chem_org,\n",
        "    x_label=\"Organ\",\n",
        "    y_label=\"PFAS\",\n",
        "    filename=\"PFAS_x_Organs_heatmap_top20x20.png\",\n",
        "    title_prefix=\"PFAS × Organs\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# PFAS × Species\n",
        "# -----------------------------\n",
        "pairs_chem_species = (\n",
        "    chem_long[[PMID_COL, CHEM_COL]]\n",
        "    .merge(species_long[[PMID_COL, SPECIES_COL]], on=PMID_COL, how=\"inner\")\n",
        "    .drop_duplicates(subset=[PMID_COL, CHEM_COL, SPECIES_COL])\n",
        "    .rename(columns={CHEM_COL: \"PFAS\", SPECIES_COL: \"Species\"})\n",
        ")\n",
        "\n",
        "ct_chem_species = (\n",
        "    pairs_chem_species\n",
        "    .groupby([\"PFAS\", \"Species\"], as_index=False)\n",
        "    .size()\n",
        "    .pivot(index=\"PFAS\", columns=\"Species\", values=\"size\")\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "plot_top20_heatmap(\n",
        "    ct_chem_species,\n",
        "    x_label=\"Species\",\n",
        "    y_label=\"PFAS\",\n",
        "    filename=\"PFAS_x_Species_heatmap_top20x20.png\",\n",
        "    title_prefix=\"PFAS × Species\"\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Species × Organs\n",
        "# -----------------------------\n",
        "pairs_species_org = (\n",
        "    species_long[[PMID_COL, SPECIES_COL]]\n",
        "    .merge(organs_long[[PMID_COL, ORGANS_COL]], on=PMID_COL, how=\"inner\")\n",
        "    .drop_duplicates(subset=[PMID_COL, SPECIES_COL, ORGANS_COL])\n",
        "    .rename(columns={SPECIES_COL: \"Species\", ORGANS_COL: \"Organ\"})\n",
        ")\n",
        "\n",
        "ct_species_org = (\n",
        "    pairs_species_org\n",
        "    .groupby([\"Species\", \"Organ\"], as_index=False)\n",
        "    .size()\n",
        "    .pivot(index=\"Species\", columns=\"Organ\", values=\"size\")\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "plot_top20_heatmap(\n",
        "    ct_species_org,\n",
        "    x_label=\"Organ\",\n",
        "    y_label=\"Species\",\n",
        "    filename=\"Species_x_Organs_heatmap_top20x20.png\",\n",
        "    title_prefix=\"Species × Organs\"\n",
        ")"
      ],
      "metadata": {
        "id": "pkZwNH9Vy7Wb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}